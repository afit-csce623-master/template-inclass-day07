{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lAsyeOwFnoX"
      },
      "source": [
        "# CSCE 623 Homework Assignment 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d-KBtJJxN1"
      },
      "source": [
        "### Student Name:  <font color=\"blue\">Enter Name</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yna07rL4Jz43"
      },
      "source": [
        "### Date: <font color=\"blue\">Enter Date</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P685WkBlKB2x"
      },
      "source": [
        "## Disclosures\n",
        "\n",
        "*   None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXzR22oMKEtf"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this homework assignment, you will play the role of a machine learning engineer. A vendor has provided you three datasets. The datasets are generically named, \"Dataset 1\", \"Dataset 2\", and \"Dataset 3.\" For each dataset, you will analyze the performance of three machine learning algorithms: logistic regression, linear discriminant analysis, and quadratic discriminant analysis, and a single tunable parameter, \"threshold.\"\n",
        "\n",
        "To support our simulated scenario, you will train and evaluate models using \"phase 1\" data for each dataset. Based on your analysis, you'll recommend a model and threshold parameter for each dataset. Finally, you'll demonstrate the performance of your recommendations on \"phase 2\" data.\n",
        "\n",
        "The goal of this homework assignment is not necessarily to attempt to maximize the performance of any model on the \"phase 2\" data. Rather, our goal is to encourage critical thinking and analysis to support high-quality decision-making using the tools you've been introduced to so far and those that you'll learn to use in this homework assignment.\n",
        "\n",
        "We've provided you with three datasets. For **EACH** dataset, you will accomplish all the steps enumerated below. ***Where appropriate, use helper functions to complete repeated tasks.***\n",
        "\n",
        "This assignment includes both written and programming components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbbSOVyq7lYk"
      },
      "source": [
        "### Written Components\n",
        "Full effort answers to written components should include not only the answer to the question, but they should also include supporting information. You should provide justification or supporting information even if the question only asks for a single number or short answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-5OaiNO7m6F"
      },
      "source": [
        "### Programming Components\n",
        "Use Python to perform any manipulations you make to provided datasets, all calculations and mathematical transformations, and to generate graphs, figures, or other support to explain how you arrived at your written answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Zjdxie-jWf"
      },
      "source": [
        "### Helpful Tips\n",
        "\n",
        "You might find these Python packages/imports helpful\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "\n",
        "from adjustText import adjust_text\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS60EAw9MiU2"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5yhrcvjN0AD"
      },
      "source": [
        "### STEP 0: installs & configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFdGmA3GN3_q"
      },
      "source": [
        "Install any packages you need for your notebook. If using the Google Colab environment, you will not need to install any additional packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqoMuH_YOJ3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b4393b1-1ef3-4d5f-eacc-802fc3ed6f9b"
      },
      "source": [
        "\"\"\"\n",
        "CSCE 623 HW2. Classification\n",
        "\"\"\"\n",
        "\n",
        "# install packages, set configuration, as needed\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCSCE 623 HW2. Classification\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HI9buTTOPHH"
      },
      "source": [
        "Import any packages you need for your notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yVNfQdnY7BW"
      },
      "source": [
        "# import pacakages for your notebook\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Lwf_jiXqXK"
      },
      "source": [
        "### Data Analysis\n",
        "\n",
        "In steps 1-2, you'll import and conduct an analysis of three datasets, `dataset1.csv`,  `dataset2.csv`, `dataset3.csv` datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwKDAUtjFnof"
      },
      "source": [
        "#### STEP 1: load datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7c6P84MOuJB"
      },
      "source": [
        "For this assignment, you will use three datasets:\n",
        "\n",
        "- hw2_dataset1.csv dataset located at https://raw.githubusercontent.com/afit-csce623-master/datasets/main/hw2_dataset1.csv\n",
        "- hw2_dataset2.csv dataset located at https://raw.githubusercontent.com/afit-csce623-master/datasets/main/hw2_dataset2.csv\n",
        "- hw2_dataset3.csv dataset located at https://raw.githubusercontent.com/afit-csce623-master/datasets/main/hw2_dataset3.csv\n",
        "\n",
        "You can view the dataset by clicking on the links above. Note that the data has a header row and a header column. Data appears in the three remaining columns. The first two data columns are features and the last data column is a class value. Load the dataset so that the columns are named `X1`, `X2`, and `Class`, respectively, discarding the raw data's original header row and column. Throughout this assignment, we will refer to the classes having a value of `0.0` as \"Class 0\" and those having a value of `1.0` as \"Class 1\".\n",
        "\n",
        "Store the datasets in `pandas` `dataframe` named `df1`, `df2`, and `df3`, respectively.\n",
        "\n",
        "For example, the `head` of `df1` will appear as follows:\n",
        "\n",
        "```\n",
        "         X1        X2  Class\n",
        "0  0.548814  0.715189    0.0\n",
        "1  0.602763  0.544883    0.0\n",
        "2  0.423655  0.645894    0.0\n",
        "3  0.437587  0.891773    0.0\n",
        "4  0.963663  0.383442    0.0\n",
        "```\n",
        "\n",
        "And the `info` of `df1` will appear as follows:\n",
        "```\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 600 entries, 0 to 599\n",
        "Data columns (total 3 columns):\n",
        " #   Column  Non-Null Count  Dtype  \n",
        "---  ------  --------------  -----  \n",
        " 0   X1      600 non-null    float64\n",
        " 1   X2      600 non-null    float64\n",
        " 2   Class   600 non-null    float64\n",
        "dtypes: float64(3)\n",
        "memory usage: 18.8 KB\n",
        "```\n",
        "\n",
        "Refer to the [`pandas.read_csv` documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_tQNDIVeFnog"
      },
      "source": [
        "#STEP 1\n",
        "\n",
        "#STUDENT CODE - insert code to load three datasets using pandas\n",
        "# store your data in a dataframes called 'df1', 'df2', and 'df3'\n",
        "#--------------------------------------------- \n",
        "\n",
        "#---------------------------------------------    \n",
        "\n",
        "STEP_1_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1HzMDO0Fnoh"
      },
      "source": [
        "#### STEP 2: plot datasets\n",
        "\n",
        "Explore each dataset by plotting datapoints in scatterplot with X1 value along the x-axis and X2 value along the y-axis. Color each datapoint according to class: one red, and the other blue.\n",
        "\n",
        "Label your plot, including x and y axes, title, and legend.\n",
        "\n",
        "For example, the plot of dataset1 might look something like this:\n",
        "\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset1_data_plot.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8o2O3DAPFnoh"
      },
      "source": [
        "#STEP 2\n",
        "\n",
        "#STUDENT CODE - Insert code to plot the datasets here\n",
        "#---------------------------------------------\n",
        "\n",
        "#--------------------------------------------- \n",
        "\n",
        "STEP_2_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzdlpIzG7Ve0"
      },
      "source": [
        "#### STEP 3: analyze datasets\n",
        "\n",
        "Continue your visual analysis of the datasets by using the `describe` functions on each dataset as well as the subset of dataset by class. Generate covariance matrices of each class of each dataset. Create relevant pairplots and histograms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMhmSFgI7TMz"
      },
      "source": [
        "#STEP 3\n",
        "\n",
        "#STUDENT CODE - Insert code to explore the dataset here\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_3_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8FUgJPGa9kg"
      },
      "source": [
        "#### STEP 4: discuss datasets\n",
        "\n",
        "Discuss each dataset. \n",
        "\n",
        "- What do you notice about the distribution of the data? \n",
        "- What can you say about the covariance of the two classes? \n",
        "- Within each class, are the variances for each feature equal? \n",
        "- Between classes, are the variances of a single feature equal? \n",
        "- How well are the classes separated? \n",
        "- For each dataset, which predictor algorithm (logistic regression, LDA, or QDA) do you think will perform best? Why?\n",
        "\n",
        "##### Dataset 1\n",
        "\n",
        "<font color=\"green\">Student Answer</font>\n",
        "\n",
        "##### Dataset 2\n",
        "\n",
        "<font color=\"green\">Student Answer</font>\n",
        "\n",
        "##### Dataset 3\n",
        "\n",
        "<font color=\"green\">Student Answer</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee7yVKzMcMP1"
      },
      "source": [
        "STEP_4_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faS4xo0gVVN_"
      },
      "source": [
        "### Prepare Data for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o9GlbhnA3tC"
      },
      "source": [
        "#### NO STEP: establish phase data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fAO3TLM1CfQ"
      },
      "source": [
        "# Do not change the following code that splits each dataset into two phases of data\n",
        "\n",
        "phase1_size = 400\n",
        "seed = 623\n",
        "\n",
        "from contextlib import suppress\n",
        "with suppress(NameError):\n",
        "    phase1_1, phase2_1 = train_test_split(df1, train_size=phase1_size, random_state=seed, stratify=df1['Class'])\n",
        "    phase1_2, phase2_2 = train_test_split(df2, train_size=phase1_size, random_state=seed, stratify=df2['Class'])\n",
        "    phase1_3, phase2_3 = train_test_split(df3, train_size=phase1_size, random_state=seed, stratify=df3['Class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDyKBJrlVGwQ"
      },
      "source": [
        "#### STEP 5: function split\n",
        "\n",
        "In this step, you'll generate a function with the signature `split(data, train_size = 0.5, random_state = 42)`.\n",
        "\n",
        "This function will take in a single dataset `data`, the percent of data that will be included in the training set `train_size`, defaulting to 0.5, and a random seed `random_state`, defaulting to 42. and return 2 `pandas` DataFrames and 2 `pandas` Series:\n",
        "- 1st DataFrame: training X values\n",
        "- 2nd DataFrame: testing X values\n",
        "- 1st Series: training y values\n",
        "- 2nd Series: testing y values\n",
        "\n",
        "The default values assume 50% of each dataset will be used for training and 50% will be used for testing. Furthermore, the class balance of the original dataset should be preserved in the phase1 and phase2 datasets. You should consider the use of the `sklearn.model_selection` `train_test_split` function.\n",
        "\n",
        "FAQ:\n",
        "- Why are we creating a `split` function when we can just call `train_test_split` directly?\n",
        "\n",
        "  The answer to this question lies at the heart of a software design principal: don't repeat yourself, or DRY. The `train_test_split` has several configuration options that you'll want to use, including a `random_state` (for reapetability purposes) and `train_size` (for the size of your training data), along with others. You'll also need to do array slicing to split the X and y data. Rather than including these in every call you make to the `train_test_split` function, you can write these once, and then call it everywhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fFQUySVTAWz"
      },
      "source": [
        "#STEP 5\n",
        "\n",
        "#STUDENT CODE - Insert code to create the split function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_5_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Owx6Mw5h-FC"
      },
      "source": [
        "#### STEP 6: split train/test data\n",
        "\n",
        "Use the `split` function you created in STEP 5 to generate 12 variables (3 function calls, 4 variables per function call):\n",
        "- X\\_train_$\\phi$\n",
        "- X\\_test_$\\phi$\n",
        "- y\\_train_$\\phi$\n",
        "- y\\_test_$\\phi$\n",
        "\n",
        "Where $\\phi \\in \\{1,2,3\\}$ represents the dataset. For example, `X_train_1` represents the portion of dataset2 used in phase1.\n",
        "\n",
        "__IMPORTANT: You should not use the `df1`, `df2`, `df3` datasets you created in STEP 1. Instead, you will use `phase1_1`, `phase1_2`, and `phase1_3` dataset created for you just before STEP 5.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G3gUwmZhrcz"
      },
      "source": [
        "#STEP 6\n",
        "\n",
        "#STUDENT CODE - Insert code to generate the 12 variables consisting of the\n",
        "# X and y values for each phase and each dataset\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_6_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6TLTmyZiUAk"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb8Z7uRkVcUv"
      },
      "source": [
        "#### STEP 7: function train\n",
        "\n",
        "Create a function `train_classifiers(X, y)` where `X` is a `pandas` `DataFrame` of feature inputs and `y` is a `pandas` `Series` of target values.\n",
        "\n",
        "The `train_classifiers` function will fit three models, logistic regression, LDA, and QDA, to the model, and return a Python dictionary such that:\n",
        "\n",
        "- the value of the key 'log' is the logistic regression model\n",
        "- the value of the key 'lda' is the LDA model\n",
        "- the value of the key 'qda' is the QDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca45sgORVeMU"
      },
      "source": [
        "#STEP 7\n",
        "\n",
        "#STUDENT CODE - Insert code to create the train_classifiers function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_7_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCorgBQkVgDA"
      },
      "source": [
        "#### STEP 8: train classifiers\n",
        "\n",
        "Using the `train_classifiers` function you created in STEP 7, create three dictionaries, `models1`, `models2`, and `models3`, which are dictionaries of models (logistic regression, LDA, and QDA) for `dataset1`, `dataset2`, and `dataset3`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUuC_UdeVgMW"
      },
      "source": [
        "#STEP 8\n",
        "\n",
        "#STUDENT CODE - Insert code to create the train_classifiers function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_8_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBd1mX4oxe4U"
      },
      "source": [
        "### Evaluate Models\n",
        "\n",
        "Having created machine learning models for each dataset in STEP 8, we will now use those models to evaluate their performance and examine the effect of candidate threshold values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0HUxi3xoZNa"
      },
      "source": [
        "#### STEP 9: function predict\n",
        "\n",
        "Create a function `predict_probabilities(models, X)` which takes in a dictionary of models that you created in STEPs 8 and 9, `models`, and a `DataFrame` `X` containing observations. The function will return a dictionary where the keys will correspond to the keys of the `models` input variable. The values of the dictionary are arrays of predicted probabilities that an observation is a member of Class 1.\n",
        "\n",
        "For example, assuming `predicts` is assigned the return value of `predict_probabilities`, `predicts['log']` is an array of probabilities that the logistic regression model of the `models` input parameter predicts each input of X is a member of Class 1. \n",
        "\n",
        "Hints:\n",
        "- Take a look at the `sklearn` `predict_proba(X)` function.\n",
        "- Note the above function returns a 2d array with the first value in each row indicating the probability the observation belongs to Class 0 with the second value being the probability the observation belongs to Class 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aq4C-NooZgi"
      },
      "source": [
        "#STEP 9\n",
        "\n",
        "#STUDENT CODE - Insert code to create the train_classifiers function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_9_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjcvSUx9wzlA"
      },
      "source": [
        "#### STEP 10: predict probabilities\n",
        "\n",
        "Using the `predict_probabilities` function you created in STEP 9, create 3 Python dictionaries, `predicts1`, `predicts2`, and `predicts3` using the models you created in STEP 8 to predict the probabilities that observations in the Phase 1 __test data__ are in Class 1.\n",
        "\n",
        "For example, `predicts2` will be a dictionary such that `predicts2['lda']` is an array of probabilities that the members of the Phase 1 test data are in Class 1 using the LDA model that was fitted on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26AD3VO9wzzA"
      },
      "source": [
        "#STEP 10\n",
        "\n",
        "#STUDENT CODE - Insert code to create the generate the \n",
        "# probability predictions for the test data\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "# print(predicts1)\n",
        "STEP_10_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSJzZoWoasax"
      },
      "source": [
        "#### STEP 11: function threshold_metrics\n",
        "\n",
        "The model \"threshold\" is a tunable parameter for the logistic regression, LDA, and QDA machine learning algorithms. These binary classifiers produce a probability that a given observation is a member of Class 0 or Class 1. The threshold parameter is a scalar value that is used to map probabilities to classes. Probabilities greater than the threshold are assigned to Class 1. By way of trivial examples, a threshold value of 0.0 results in nearly all observations being classified as Class 1, whereas as a threshold value of 1.0 results in no observations being classified as Class 1.\n",
        "\n",
        "In this step, you will create `threshold_metrics(y_true, y_probs, threshold)` where `y_true` and `y_probs` are `numpy` `ndarray`s representing the true class values and model's prediction probabilities, respectively. The `threshold` is a Python `list` of candidate thresholds for evaluation.\n",
        "\n",
        "The function `threshold_metrics` will return a dataframe with $m$ rows and 10 columns, where $m$ is the length of the `thresholds` list. The columns will be:\n",
        "- Threshold\n",
        "- True Positive\n",
        "- True Negative\n",
        "- False Negative\n",
        "- Recall\n",
        "- Precision\n",
        "- False Positive Rate\n",
        "- Accuracy\n",
        "- F-measure\n",
        "\n",
        "Each row will include one of the candidate threshold from the `thresholds` list and the model's performance metrics when that threshold value is used to determine the class. See the following example of sample inputs and expected `DataFrame` output.\n",
        "\n",
        "Sample true values:\n",
        "```\n",
        "[0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
        "```\n",
        "\n",
        "Sample model probabilities:\n",
        "```\n",
        "[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864 0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
        "```\n",
        "\n",
        "And thresholds:\n",
        "```\n",
        "[0.5 0.75]\n",
        "```\n",
        "\n",
        "`DataFrame` yielded by `threshold_metrics` function:\n",
        "```\n",
        "   Threshold  True Positive  False Positive  True Negative  False Negative       Recall  Precision  False Positive Rate  Accuracy  F-measure\n",
        "0       0.50            3.0             3.0            4.0             0.0     1.000000        0.5             0.000000       0.7   0.666667   \n",
        "1       0.75            2.0             0.0            7.0             1.0     0.666667        1.0             0.333333       0.9   0.800000  \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_3K8g40Lc9"
      },
      "source": [
        "#STEP 11\n",
        "\n",
        "#STUDENT CODE - Insert code to create the threshold_metrics function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_11_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5iHJJbHF7-1"
      },
      "source": [
        "#### EXTRA: function display_threshold_metrics\n",
        "\n",
        "You may use the following code to test threshold_metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eww1TpNwjFZ5"
      },
      "source": [
        "### Code to generate sample data for STEP 11 instructions\n",
        "DEBUG = True # this DEBUG flag allows you to run this code only when debugging\n",
        "if DEBUG:\n",
        "    from contextlib import suppress\n",
        "    with suppress(NameError):\n",
        "        np.random.seed(42)\n",
        "        y_probs = np.random.random(10)\n",
        "        truths = np.around(np.random.random(10))\n",
        "        thresholds = [0.5, 0.75]\n",
        "\n",
        "        vals = threshold_metrics(truths, y_probs, thresholds)\n",
        "\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "            print('Truth values: ', truths)\n",
        "            print('Predicted probabilities: ', y_probs)\n",
        "            print('Thresholds: ', thresholds)\n",
        "            display(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixh5XyEJB69z"
      },
      "source": [
        "#### STEP 12: function thresholds_argmax \n",
        "\n",
        "In this step, you'll create a function, `thresholds_argmax(y, predicts)` which receives parameters `y`, an array of truth values, and `predicts`, a dictionary of models and arrays of probabilities that observations belong to Class 1.\n",
        "\n",
        "This function will return the threshold data for the thresholds where each of the following metrics are maximized: Accuracy, Precision, Recall, and F-measure.\n",
        "\n",
        "For example, if `thresholds1` contains the return value from the function applied to Dataset 1, `thresholds1['log']['Accuracy']` will contain the threshold that yields the best accuracy of the logistic regression model along with all the other metrics returned by threshold data, for example:\n",
        "\n",
        "```\n",
        "Threshold                0.298085\n",
        "True Positive          137.000000\n",
        "False Positive          60.000000\n",
        "True Negative           90.000000\n",
        "False Negative          13.000000\n",
        "Recall                   0.913333\n",
        "Precision                0.695431\n",
        "False Positive Rate      0.086667\n",
        "Accuracy                 0.756667\n",
        "F-measure                0.789625\n",
        "Name: 74, dtype: float64\n",
        "```\n",
        "\n",
        "In this way, `thresholds1['log']['Accuracy']['Threshold']` is the threshold value where accuracy is maximized using a logistic regression model on Dataset 1 and `tresholds1['log']['Accuracy']['Accuracy']` is the highest accuracy.\n",
        "\n",
        "Hints:\n",
        "- The `roc_curve` method of the `sklearn.metrics` pacakage takes in truth values and probabilities and returns the false positive rate, true positive rate, and thresholds for evaluation. You may find the thresholds returned by this function useful for further evaluation to find the threshold values that maximize the metrics of interest.\n",
        "- The `numpy` `argmax(a)` function returns the index of the array `a` of the maximum value of `a`.\n",
        "- Thus, `data.iloc[np.argmax(data[col])]` will return the row where `data[col]` is maximized.\n",
        "- Your `threshold_metrics` function may raise errors or warnings if a candidate threshold is selected such that both precision and recall are 0, since the sum of these is the denominator of F-measure. To suppress these warnings, you can call `threshold_metrics` in the following context: `with np.errstate(divide='ignore', invalid='ignore'):` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-WXBLPzUscr"
      },
      "source": [
        "#STEP 12\n",
        "\n",
        "#STUDENT CODE - Insert code to create the thresholds_argmax function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_12_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce5CJ-mE6dS"
      },
      "source": [
        "#### EXTRA: function display_thresholds_argmax\n",
        "\n",
        "These functions are provided to display the threshold data created by `thresholds_argmax`. You may also use `display_thresholds_argmax` to analyze any arbitrary set of thresholds elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGXSF-oeKdc3"
      },
      "source": [
        "\n",
        "def display_thresholds_argmax(y, predicts, title):\n",
        "    vals = thresholds_argmax(y, predicts)\n",
        "\n",
        "    for method, threshold_metrics in vals.items():\n",
        "        display(md(title + ' ' + method.upper()))\n",
        "\n",
        "        df = pd.DataFrame()\n",
        "        for metric, values in threshold_metrics.items():\n",
        "            df = df.append(values.to_frame().T)\n",
        "\n",
        "        df['Metric'] = threshold_metrics.keys()\n",
        "        df = df.set_index('Metric', drop=True)\n",
        "        df.index.name = None\n",
        "        display(df)\n",
        "\n",
        "from contextlib import suppress\n",
        "with suppress(NameError):\n",
        "    display_thresholds_argmax(y_test_1, predicts1, 'Dataset 1')\n",
        "    display_thresholds_argmax(y_test_2, predicts2, 'Dataset 2')\n",
        "    display_thresholds_argmax(y_test_3, predicts3, 'Dataset 3')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLmhuIkbJd3z"
      },
      "source": [
        "#### EXTRA: functions for numerical and visual analysis\n",
        "\n",
        "These functions may optionally be used to complete STEP 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EemwOHu4JdC0"
      },
      "source": [
        "def display_thresholds(y, predicts, method):\n",
        "    _, _, thresholds = roc_curve(y, predicts[method])\n",
        "    \n",
        "    display(md('Thresholds of %s model' %(method.upper())))\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None): # show all rows and columns\n",
        "        display(threshold_metrics(y, predicts[method], thresholds))\n",
        "\n",
        "\n",
        "def plot_threshold_on_roc_curve(y, predicts, selection):\n",
        "    vals = threshold_metrics(y, predicts[selection['method']], [selection['threshold']])\n",
        "    display(md('%s model threshold = %.2f' %(selection['method'].upper(), selection['threshold'])))\n",
        "    display(vals)\n",
        "    vals = np.squeeze(vals)\n",
        "    plt.plot([-0.05, vals['False Positive Rate']], [vals['Recall'], vals['Recall']], '--r', linewidth=1, label='%s threshold = %.2f' %(selection['method'].upper(), selection['threshold']))\n",
        "    plt.plot([vals['False Positive Rate'], vals['False Positive Rate']], [-0.5, vals['Recall']], '--r', linewidth=1)\n",
        "\n",
        "\n",
        "def plot_threshold_on_precision_recall_curve(y, predicts, selection):\n",
        "    vals = threshold_metrics(y, predicts[selection['method']], [selection['threshold']])\n",
        "    display(md('%s model threshold = %.2f' %(selection['method'].upper(), selection['threshold'])))\n",
        "    display(vals)\n",
        "    vals = np.squeeze(vals)\n",
        "    plt.plot([-0.05, vals['Threshold']],[max(vals[\"Precision\"], vals[\"Recall\"]), max(vals[\"Precision\"], vals[\"Recall\"])], '--r', linewidth=1, label='%s threshold = %.2f' %(selection['method'].upper(), selection['threshold']))\n",
        "    plt.plot([-0.05, vals['Threshold']],[min(vals[\"Precision\"], vals[\"Recall\"]), min(vals[\"Precision\"], vals[\"Recall\"])], '--r', linewidth=1)\n",
        "    plt.plot([vals['Threshold'], vals['Threshold']], [-0.05, max(vals[\"Precision\"], vals[\"Recall\"])], '--r', linewidth=1)\n",
        "\n",
        "\n",
        "def plot_threshold_on_precision_vs_recall_curve(y, predicts, selection):\n",
        "    vals = threshold_metrics(y, predicts[selection['method']], [selection['threshold']])\n",
        "    display(md('%s model threshold = %.2f' %(selection['method'].upper(), selection['threshold'])))\n",
        "    display(vals)\n",
        "    vals = np.squeeze(vals)\n",
        "    plt.plot([-0.05, vals['Recall']],[vals['Precision'],vals['Precision']], '--r', linewidth=1, label='%s threshold = %.2f' %(selection['method'].upper(), selection['threshold']))\n",
        "    plt.plot([vals['Recall'], vals['Recall']], [-0.05, vals['Precision']], '--r', linewidth=1)\n",
        "\n",
        "\n",
        "\n",
        "def set_plot_size(width=12, height=9):\n",
        "    plt.figure(figsize=(width, height))\n",
        "\n",
        "\n",
        "def set_plot_defaults(title, **kwargs):\n",
        "    legend_loc = kwargs.get('legend_loc', 'lower left')\n",
        "    plt.xlim([-0.01, 1.01])\n",
        "    plt.ylim([-0.01, 1.01])\n",
        "    plt.grid(True)\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=legend_loc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tk2BK57hViC"
      },
      "source": [
        "#### STEP 13: confusion matrices\n",
        "\n",
        "Print confusion matrices for each of the threshold values that maximize the metrics of interest noted in STEP 11 (Accuracy, Precision, Recall, and F-Measure). Each confusion matrix should have a text header indicating which metric is being maximized and what the threshold value is. See below for an example.\n",
        "\n",
        "Hints:\n",
        "- In total, you will create 36 confusion matrices (3 datasets with 3 models each, and 4 metrics).\n",
        "- You may use the `confusion_matrix` function of the `sklearn.metrics` package, or you may calculate the values yourself.\n",
        "- You can take advantage of broadcasting to manipulate a probabilities array into the resulting predictions. For example, if the threshold value is `0.3`, and you have a `predicts2` dictionary, `predicts2['qda'] > 0.3` will create an array of True and False values referring to whether the observation is predicted to belong in Class 1, based on the threshold value.\n",
        "- Storing values in a DataFrame, then displaying that DataFrame, can be a convenient way of displaying tables. \n",
        "- A good design practice may involve creating two functions:\n",
        "  - `display_confusion_matrix(confusion_data)` which takes in a 2x2 array, adds index and column headers, and then displays it\n",
        "  - `generate_confusion_matrices(y, predicts, title)` which will be called three times, once for each dataset. It will generate confusion table data and call `display_confusion_matrix` for every model and metric of interest\n",
        "- Here is an example of what the confusion matrices for the logistic regression model applied to Dataset 1 might look like.\n",
        "\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset1_log_conf_matrix.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNgq4jTkhV7z"
      },
      "source": [
        "# STEP 13\n",
        "\n",
        "#STUDENT CODE - Insert code to create the threshold_metrics function\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_13_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fxbKZ7cG9dL"
      },
      "source": [
        "#### STEP 14: plot roc curves\n",
        "\n",
        "For each dataset, plot the Receiver Operating Characteristics (ROC) curve for each classifier. You'll plot all three classifiers in a different color on the same plot. Include the AUC calculations with your plot\n",
        "\n",
        "Annotate each plot to represent the points of maximum accuracy, precision, recall, and F-measure for each model. OPTIONAL: use the Python package `adjustText` to automatically displace the labels and to add arrows to the locations of interest along the ROC curves. Moreover, these [examples](https://adjusttext.readthedocs.io/en/latest/Examples.html) may also be helpful. Our adjust_text parameters closely resemble the first of the \"Difficult examples\" on this page, though we increased the values for `expand_text` and `expand_points`.\n",
        "\n",
        "Hints:\n",
        "- Consider a single function with the signature `plot_roc_curves(y, predicts, title)` which will be called three times for each dataset. `y` is your test set y-values, `predicts` is the dictionary you created in STEP 10, and `title` is the title of the plot. Thus, `plot_roc_curves` will create a single plot with the roc_curve of three models for that dataset. You would call `plot_roc_curves` three times, once for each dataset.\n",
        "- You may use either or both the `sklearn.metrics` `plot_roc_curve` and `roc_curve`. If you choose to use `plot_roc_curve` you'll need to store the plt object to overlay subsequent curves. `plot_roc_curve` automatically calculates and displays the `auc` metric. The instructor's solution does not use `plot_roc_curve`.\n",
        "- Instead of generating `fpr`, `tpr`, and `threshold` values from `roc_curve`, you may alternatively create your own array of threshold values using `linspace`. You would then use the `threshold_metrics` function you created in STEP 11 on the array and extract the relevant metrics for your plot. This is the method the instructor's solution uses, as it produces a smoother plot (we used 100 intervals).\n",
        "- You may use the `roc_auc_score` function from `sklearn.metrics`\n",
        "- Pages 93-99 of the _HOML_ text may provide helpful Python code examples.\n",
        "- Here is an example plot from the instructor's soluton\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset2_roc_plot.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMgK2x4MI6Y0"
      },
      "source": [
        "#STEP 14\n",
        "\n",
        "#STUDENT CODE - Insert code to generate the ROC curves\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_14_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljXm_sPzGYK3"
      },
      "source": [
        "#### STEP 15: plot precision recall curves\n",
        "\n",
        "Plot precision and recall vs threshold values. The y-axis of your plot will be precision/recall values $[0.0, 1.0]$ and the x-axis will be threshold values $[0.0, 1.0]$. You will create one plot per dataset and add precision/recall curves for each model. Use line styling to visually associate precision/recall curves with model and type. For example, you could use a different color for each model and a consistent line style for all precision curves.\n",
        "\n",
        "Hints:\n",
        "- Consider a single function with the signature `plot_precision_recall_vs_threshold(y, predicts, title)`. `y` is your test set y-values, `predicts` is the dictionary you created in STEP 10, and `title` is the title of the plot. Thus, `plot_precision_recall_vs_threshold` will create a single plot with the precision/recall curve of three models for that dataset. You would call `plot_precision_recall_vs_threshold` three times, once for each dataset.\n",
        "- You may use the `sklearn.metrics` `precision_recall_curve` function to generate your precision, recall, and threshold values.\n",
        "- Pages 93-99 of the _HOML_ text may provide helpful Python code examples\n",
        "- Here is an example plot from the instructor's solution:\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset2_precision_recall_vs_thresholds.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Zb9vg_MtMK"
      },
      "source": [
        "#STEP 15\n",
        "\n",
        "#STUDENT CODE - Insert code to generate the Precision/Recall vs Threshold curves\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_15_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLmU9ni07R9L"
      },
      "source": [
        "#### STEP 16: plot recall vs precision\n",
        "\n",
        "Plot precision vs recall. The x-axis of your plot will be recall values $[0.0, 1.0]$ and the y-axis will be precision values $[0.0, 1.0]$. You will create one plot per dataset and add precision vs recall curves for each model. Use one color per model.\n",
        "\n",
        "Hints:\n",
        "- Consider a single function with the signature `plot_precision_vs_recall(y, predicts, title)`. `y` is your test set y-values, `predicts` is the dictionary you created in STEP 10, and `title` is the title of the plot. Thus, `plot_precision_vs_recall` will create a single plot with the precision vs recall curve of three models for that dataset. You would call `plot_precision_recall_vs_threshold` three times, once for each dataset.\n",
        "- You may use the `sklearn.metrics` `precision_recall_curve` function to generate your precision, recall, and threshold values.\n",
        "- Pages 93-99 of the _HOML_ text may provide helpful Python code examples\n",
        "- Here is an example plot from the instructor's solution:\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset1_precision_vs_recall_plot.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqvp9gXr7Tw6"
      },
      "source": [
        "\n",
        "#STEP 16\n",
        "\n",
        "#STUDENT CODE - Insert code to generate the precision vs recall curves\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_16_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGzshCeD5247"
      },
      "source": [
        "### Model Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWxZ1CWl6PEj"
      },
      "source": [
        "#### STEP 17\n",
        "\n",
        "Using the analysis tools you created in steps 13-16, explain the tradeoff that is occuring when threshold values are selected to maximize accuracy, precision, recall, and F-measure. The discussion you provide here is not intended to be an exhaustive analysis of these datasets and these models in particular. Instead, discuss the general characteristics of these tradeoffs.\n",
        "\n",
        "<font color=\"green\">Student Answer</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV6M9AEuUnZw"
      },
      "source": [
        "STEP_17_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LSPsrH_7RV2"
      },
      "source": [
        "#### STEP 18\n",
        "\n",
        "#### Written Response\n",
        "\n",
        "For each dataset, describe which model and threshold value you would recommend using? Why? If it helps you develop your answer, you may be creative with what the data in each dataset represents. For example, you could say, \"We will assume Dataset 1 represents the probability an individual will be promoted to the next rank based on features inputs of height and the number of years in service. Therefore... \"\n",
        "\n",
        "Additionally, a previous section (NO STEP: functions for numerical and visual analysis, prior to STEP 13) provides some tools that you may find helpful in your analysis.\n",
        "\n",
        "- `display_thresholds(y, predicts, method)` will display all relevant threshold and metrics for a particular model. `y` is the test y-values, `predicts` is a predict model you created in STEP 10, and `method` is a string `{'log', 'lda', 'qda'}` referring to the model for which you want to list threshold values and metrics.\n",
        "- `plot_threshold_on_xxxxxx(y, predicts, selection)` is a set of functions that will overlay a threshold value on a plot. You can the appropriate function into the plot functions you created in steps 14-16. `selection` is a dictionary with two keys, `{method, threshold}`, where `method` is the model `{'log', 'lda', 'qda'}` you want to analyze and `threshold` is a desired threshold. For example, inserting\n",
        "\n",
        "  `plot_threshold_on_precision_recall_curve(y, predicts, {'method': 'lda', 'threshold': 0.5)` \n",
        "\n",
        "  into your plot function in STEP 15 will add lines through precision and recall curves at the threshold value of 0.5. The function will also display the relevant metrics at the threshold value of 0.5.\n",
        "\n",
        "\n",
        "#### Student Answers\n",
        "\n",
        "- Dataset 1\n",
        "\n",
        "  <font color=\"green\">Student Answer</font>\n",
        "\n",
        "\n",
        "- Dataset 2\n",
        "\n",
        "  <font color=\"green\">Student Answer</font>\n",
        "\n",
        "\n",
        "- Dataset 3\n",
        "\n",
        "  <font color=\"green\">Student Answer</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Dm3zP5UrgN"
      },
      "source": [
        "STEP_18_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnwsd4HCVQI"
      },
      "source": [
        "### Optional: Evaluate Delivered Model & Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osxGFKKCCSRo"
      },
      "source": [
        "#### OPTIONAL STEP 19: evaluate delivered model\n",
        "\n",
        "Apply the model and threshold paramater you chose in STEP 18 to the vendor's phase 2 data. The phase 2 data is available in variables `phase2_1`, `phase2_2`, and `phase2_3`. As you are simply evaluating your models on phase 2 data, there is no need to split the phase 2 data into train and test cases, though you will want to split the data into X inputs and y outputs. You will then use your model to make predictions or conduct \"inference\" on the phase 2 dataset, applying your chosen threshold. You'll evaluate the precision, recall, f-measure, and confusion matrices of your model as applied to the Phase 2 dataset. Finally, discuss its similarity and differences to the performance of your model on phase 1 data.\n",
        "\n",
        "Hints:\n",
        "- To apply your trained model to the phase 2 data, you'll should execute the following concrete steps:\n",
        "  - Split `phase2_1` data into `X_phase2_1` and `y_phase2_1`, repeat for datasets 2 and 3\n",
        "  - Use the appropriate model you created in STEP 8, designating the specific classifier you chose in STEP 18 to use to make predictions. For example, if you decided to use the 'LDA' model for Dataset 3, you would use `models3['lda']` to predict probabilities. Recall that `predict_proba` returns a 2-column array consisting of the probabilities the observation belongs on Class 0 and Class 1. For binary classification, you'll only need the probability that the observation belongs in Class 1.\n",
        "  - Apply the `threshold` parameter you chose in STEP 18. Broadcasting is a help here. For example, if `phase2_1_predicts` is an array of predictions on Dataset 1 using the model chosen in the previous step, `phase2_1_predicts > threshold` will return an array of `True` and `False` values indicating whether your model predicts that a given an observation is in Class 0 (`False`) or Class 1 (`True`).\n",
        "  - Generate the precision, recall, f-measure, and confusion matrices by comparing the class assignment array you created in the previous action with the truth values in `y_phase2_1` (or alternative y-values for Dataset 2 or 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJAWf4I7T6H"
      },
      "source": [
        "#STEP 19\n",
        "\n",
        "#STUDENT CODE - Insert code to evaluate the performance of the chosen\n",
        "# models on Phase 2 data\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWz4zmzKCZ-f"
      },
      "source": [
        "Discuss the performance of your model on Phase 2 data compared with its performance on Phase 1 data. Is the performance similar? Why or why not?\n",
        "\n",
        "<font color=\"green\">Student Answer</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAn3LgG-U8fC"
      },
      "source": [
        "STEP_19_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOs6BoCWDYxO"
      },
      "source": [
        "#### OPTIONAL STEP 20: visualize model\n",
        "\n",
        "For each dataset (`df1`, `df2`, `df3`), plot your chosen model overlayed with all the data in the dataset. Display decision boundaries to show how the model will classify the dataset. Feel free to use [this answer](https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression) at Stack Overflow for inspiration.\n",
        "\n",
        "This is an sample QDA model applied to Dataset 3.\n",
        "\n",
        "<img src='https://github.com/afit-csce623-master/template-hw2/blob/main/images/dataset3_model_and_data.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUmpARFdeICu"
      },
      "source": [
        "#STEP 20\n",
        "\n",
        "#STUDENT CODE - Insert code to plot the classifier overlayed with the dataset\n",
        "#---------------------------------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "\n",
        "STEP_20_COMPLETE = False"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}